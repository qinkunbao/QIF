\section{Background}
In this section, we first present a basic introduction about the 
memory-based side-channel attack. Those attacks 
are exactly what we attempt to study in the paper. After that we 
will present existing work on side-channel detection and quantification.
We will also analyze strengths and limitations of those quantification 
methods.

\subsection{Address-based Side-Channels}
Address-based side-channels are information channels that can leak sensitive information unintended
through the different behaviors when the program accesses different memory addrsss. Fundamentally,
those difrences were caused by the memory hierarchy design in modern computer systems. When the 
CPU fetches the data, it will first search the cache, which stores copies of the data from 
the frequently used main memory. If the data doesn't exist in the cache, the CPU will read
the data from the main memory (RAM). Classified by the layer caused the side-channel, we 
introduce two kinds of commom side-channels: cache-based side-channel attacks and memory-based
side-channel attacks.

\subsubsection{Cache-based Attack}
In general, the cached-based side-channel attacks seek information 
rely on the time differences between the cache miss
and cache hit. Here we introduce two types of cache attacks:
PRIME+PROBE, FLUSH+RELOAD.

\textbf{PRIME+PROBE} targets a single cache set. It has two phases. During the
"prime" phase, the attacker fills the cache set will his own data.
In the second "probe" phase, the attacker accesses the cache set
again. If the victim accesses the cache set and evicts part of 
the data, the attacker will experience a slow mesurement. If not, 
the mesurement will be fast.

\textbf{FLUSH+RELOAD} targets a single cache line. 
It requires the attacker and victim share the same memory.
It also have two phases. During the "flush" phase, the attacker 
will flush the "monitered memory" from the cache. Then the attacker
wait for the victim to access the memory. In the third phase, the 
attacker reload the "monitered memory". If the time is short, which
indicates there is a cache hit and the victim reolads the memory before. 
On the other hand, the time will be longer since the CPU need to reolad
the memory into the cache line. 

\subsubsection{Memory-based Attack}
Memory-based side-channel attack\cite{} exploits the different behaviors when the
program accesses different page tables. The controlled-channel attack\cite{7163052},
which works in the kernel space, can infer the sensitive data in the shielding systems by
observing the page fault sequences by restricting some code and
data pages. 

After examing the memory-based side-channels attack. We find the fundamentally
reason of those attacks are due to secret-dependent memory access and control
flow transfers.
\lstinputlisting[language=c, 
                 numbers=right,
                 caption={Sample code shows secret-dependent memory access and 
                          secret-dependent control-flow transfer.},
                 captionpos=b,
                 label={code:background},
                 basicstyle=\fontsize{7}{9}\selectfont\ttfamily]
                 {sample_code/background.c}

For exampls, the above code~\ref{code:background} show an simple encryption function that
has the two kinds of side-channels. At the line 11, dependending on the value of key,
the code will access the different entry in the predefined table /textbf{Table}. At the
line 13, the code will do a series of computation and determine if the code in the if
branch is executed or not. Such vulnerabilities could leak to the memory-based 
side-channles. We identify and quantify the leakage of the two kinds of vulnerabilities 
in the paper.

\subsection{Information Leakage Quantification}
Given an event e which occurs with the probability $P(e)$, if the event e happens, 
then we receive
\begin{equation}
    I = - log(P(e))
\end{equation}
bits of information by knowing the event e.

The above definition is obvious. Suppose a char variable \textit{a} in C program has the size
of one byte (8 bits), so the value in the variable can range from 0 - 255. We assume
the \textit{a} has the uniform distribution. If at one time we observe the \textit{a}
equals to 1, the probability will be 1/256. So the information we get is 
$-log(1/256) = 8 bits$, which is exactly the size of the char variable in C program.

Existing works on information leakage quantification are based on mutual information or 
min-entropy \cite{10.1007/978-3-642-00596-1_21}.
In their frameworks, the input sensitive
information $K$ is viewed as random variables. Let $k_i$ be one of the possible
value of $K$. The Shannon entropy $H(K)$ is defined by
\begin{equation}
    H(K) = - \sum_{k_i {\in} K}P(k_i)log(P(k_i))
\end{equation}

The Shannon entropy can be used to quantify the initial uncertainty about the sensitive
information. Suppose a program (P) with the $K$ as
the sensitive input, an adversary has some observations (O) through the side-channels.
In this work, the observations are referred to the secret-dependent control-flows and
secret-dependent data-accesses patterns. The conditional entropy $H(K|O)$ is
\begin{equation}
    H(K|O) = - \sum_{o_j {\in} O} {P(o_j) \sum_{k_i {\in} K}{P(k_i|o_j)log(P(k_i|o_j))}}
\end{equation}
Intuitively, the conditional information marks the uncertainty about $K$ after the adversary
has gained some observations (O). 

Many previous works use the mutual information $I(K; O)$ to quantify the leakage which is defined 
as follows:
\begin{equation}
    Leakage = I(K;O) = \sum_{k_i {\in} K}{\sum_{o_j {\in} O}{P(k_i, o_j)log(\frac{P(k_i, o_j)}{P(k_i)P(o_j)})}}
\end{equation}
where $P(k_i, o_i)$ is the joint discrete distribution of $K$ and $O$.
Alternatively, the mutual information can also be computed with the following equation:
\begin{equation}
    Leakage = I(K;O) = H(K) - H(K|O) = H(O) - H(O|K)
\end{equation} 
For a deterministic program, once the input $K$ is fixed, the program will have the same
control-flow transfers and data-access patterns. As a result, $P(k_i, o_j)$ will always
equals to 1. So the conditional entropty $H(O|K)$ will equal to zero. Now the leakage defined
by the mutual information can be simplified into:
\begin{equation}
    Leakage = I(K;O) = H(O)
\end{equation}
In other words, once we know the distribution of those memory-access patterns. We can 
calculate how much information is actually leaked.

Another common method is based on maximal leakage ~\cite{10.1007/978-3-642-00596-1_21,10.1007/978-3-642-31424-7_40,182946}.

\begin{equation}
    Leakage = log(C(O))
\end{equation}
$C(O)$ represents the number of different observations that an attacker can have.

Now we provide a concrete example to show how the two types of quantification definition works.
\lstinputlisting[language=c, 
                 numbers=right,
                 caption={A simple program},
                 captionpos=b,
                 label={code::entropy},
                 basicstyle=\fontsize{7}{9}\selectfont\ttfamily]
                 {sample_code/dependent.c}

\textbf{Maximal leakage} 
Dependenting on the value of key, the code can run four different branches which corrosponding to 
four different observations. Therefore, by the maximal leakage definition, the leakage equals to 
$log4 = 2$ bits.

\textbf{Mutual Information} If the key satisfies the uniform distribution, the probability of the code runs each branch
can be computed with the following result: 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Branch & A     & B      & C      & D       \\ \hline
P      & 1/256 & 64/256 & 64/256 & 127/256 \\ \hline
\end{tabular}
\caption{The distribution of observations}
\end{table}
Therefore, the leakage equals to 
$\frac{1}{256}log\frac{1}{256} + \frac{1}{4}log\frac{1}{4}*2 + \frac{127}{256}log\frac{127}{256} = 1.7$ bits.
