\section{Introduction}

Side channels are inevitable in modern computer systems. The attacker can infer the sensitive 
information by observing the execution behaviour. The software-based attacks(e.g., Cache-based 
side channels~\cite{184415,191010,7163050,Osvik:2006:CAC:2117739.2117741}, 
controlled channel attacks~\cite{7163052}) 
are especially common because these attacks typically don't need any physical access. 
After examing the root cause of those vulnerabilities, we believe that the implementation 
of those software, which has different memory access patterns depending on the input secret 
during the execution, cause the following address-based side channels. Those side-channel
vulnerabilities can be exploited by observing the different memory access patterns. An
attacker can observe the pattern and infer the sensitive information.

Various countermeasures~\cite{182946,203878,217537} have been proposed to defend the 
address-based side-channel attacks. The basic idea is simple, identifying 
and eliminating secret-dependent memory access patterns. 
However, it can be very tedious and error-prone to find every the leakage site manually. 
To address the problem, a lot of methods have been proposed to automatically detect information leakages. 
While those tools can typically report a list of potential leakages sites, they still 
face the following limitations.

First, existing tools~\cite{203878, 217537} can discover a list of potential leakage sites. 
But they fail to report how severe each potential leakage site could be. Most of leakages 
found by those tools are typically hard to be exploited and leak very little information 
(e.g., one bit of the length of the key~\cite{203878}).  
It is worthwhile to have a tool to tell how much information is actually leaks. 
For example, some secret dependent memory access patterns in OpenSSL have been known 
and studied for years. But the authors still don't fix them all because they think 
those vulnerabilities aren’t not severe enough. The tool based on static analysis~\cite{182946} with 
abstract interpretation, can give inforamtion leakage upper bound, which is useful to 
justify the implementation is secure enough. But they can’t indicate the leakage site 
is severe or not due to the over approximation when those tools calculate inforamtion leakage. 
The dynamic method will take a concrete input and run the actual program. 
Those tools are typically very precise in term of true leakage. But none of those tools 
can actually report how many bits are actually leaked. For example, DATA~\cite{217537} reports more 
than 2000 potential leakage sites for the RSA implementation of OpenSSL. But most of 
them were dismissed by the author after some manual inspections.

Second, many open source libraries may have multiple leakage sites. 
A side-channel attacker~\cite{191010,7163052} can exploit more than one leakage sites at one time. 
The attacker may retrieve some information from one site and some other information 
from another site.
It is hard know how much information is actually leaked in total. Adding those leakages 
simply still gets an upper bound estimate of the total information leakage if those 
leakages aren’t independent. No existing tools can practically estimate the total information 
leakage from multiple leakage sites in real world open source libraries.

To overcome the above limitations, we propose a novel method
to more precisely quantify the information leakage. Previous work only considers the
``average'' information leakage which often neglect severe scenarios.
% The average information leakage assume that an event, e.g., a branch condition,
% can either happen or not happen in the future,
% which often does not match the attach scenario..
In contrast, our method is precise and fine-grain. 
For example, given a crypto key, we can assume the key is fixed
in the attack scenario because the attacker can fix the key and perform profiling.
The average leakage does not fit in this scenario.
The key insight is ...  \fixme{Qinkun: complete this and i will revise.}
By fixing with more constraints, we are able to obtain more precise information leakage.
Our results are superisingly different compared to previous results
and much more useful in practice.
Our method can also precisely combine two leakages. 

Our method can automatically identify and quantify address-based
sensitive information leakage sites in real-world applications. The intuition is that most
adversaries will exploit different control-flow transfer and data-access patterns when 
the program processes different sensitive data. We refer them as the potential information
leakage sites. Our tool can provide a list of potential information leakage sites as well as how many 
bits they can leak. We are also able to report precisely how many bits in total
can be leaked if an attacker is able to observe more than one site.

We define the amount of leaked information in a simple but effective way. It is interesting to 
mention that the definition here is different from the definition in several static analysis 
tools.
Our method is based on ... \fixme{Qinkun: explain in high level, but give a novelty sense on
  how our method work, the method or theory, not workflow, not implementation.}
We first run symbolic 
execution on execution traces. We model each side-channel leakage as a logic formula. 
The sensitive input is divided into several independent bytes and each byte is regarded as 
a unique symbol. Those formulas can precisely model every the side-channel vulnerability. 
In other words, if the application has a different sensitive input but still satisfies the formula, 
the code can still leak the same information about the input.  
Those information leakage sites may spread in the whole program 
and their leakages may not be dependent. Simply adding them up can only give an upper bound 
estimate. In order to accurately calculate the total information leakage, we must know the 
dependent relationships among those multiple leakages sites. Therefore, we introduce a 
monte carlo sampling method to estimate the total information leakage.

We implement the proposed method as a tool named \tool{} which can precisely discover and quantify the information
leakage vulnerabilities. We apply \tool{} on several crypto and non-crypto libraries including OpenSSL,
MbedTLS and libjpeg. The experiment result confirms that \tool{} can precisely identify the pre-known vulnerabilities,
report how much information is leaked and which byte in the original sensitive buffer is leaked. 
The result shows that while those crypto libraries have a number of side-channels, most of them actually
leak very little inforamtion. Also, we also use the tool to analysis the two reported side-channel attack 
in the libjpeg library. Finally, we present new vulnerabilities. With the help of \tool{}, we confirm those
vulnerabilities are easily to be exploited.

In summary, we make the following contributions:

\begin{itemize}
	\item We propose a novel method that can quantify fine-grain side-channel
        information leakages. We model each information leakage vulnerability as a formula and 
        mutiple side-channel vulnerabilities can be seen as the disjunctions of those formula, which
        precisely models the program semantics.
        \item We transfer the information quantification problem into a probabilty distribution problem and 
        use the Monte Carlo sampling method to estimate the information leakage. %% Some initial results indicate the 
        %% the sampling method suffers from the curse of dimensionality problem. We therefore design a guided
        %% sampling method and provide the corresponding error esitimate.
	\item We implement the proposed method into a practical tool and apply it to several real-world software. \tool{} 
        successfully identify the address-based side-channel vulnerabilities and provide the corresponding
        information leakge. The information lekage result provide the detailed information that help developers
        to fix the vulnerability.
\end{itemize}



