\subsection{Information Leakage Estimation}

\newcommand{\addr}[1]{{l}_{#1}}
\renewcommand{\addr}[1]{{\gamma}_{#1}}
\renewcommand{\addr}[1]{{\zeta}_{#1}}
\renewcommand{\addr}[1]{{\xi}_{#1}}

In this section, we present the alogorithm to calculate the information
leakage based on the definition~\ref{def}. 

\subsubsection{Problem Statement}
From the above step~\ref{InstructionSE}, we can generate the constraint 
for each unique leakage site on the execution trace.
The only variables in those constraints are the sensitive data represented
with $k_1, k_2, \ldots , k_n$. Suppose the address of the leakage site is $\addr{i}$,
we use $C_{\addr{i}}$ to denote the constraint. For multiple leakage sites, 
as each leakage site has one constraint, we 
use the conjunction of those constraints to represent those leakage sites. 

According to the definition~\ref{def}, to calculate the amount of leaked 
information, the key is calculating $\frac{|K|}{|K^o|}$. $K^o$ represents
the set that contains every input keys that satisfy the constraint. As the 
cardinality of $K$ is known, the key problem is to know the cardinality of
$K^o$. Suppose an attacker can observe $n$ leakage sites, and each leakage site has
the following constraints: $C_{\addr{1}}, C_{\addr{2}}, \ldots, C_{\addr{n}}$ respectively. 
The total leakage has the constraint 
$F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}}) = C_{\addr{1}} \land C_{\addr{2}} 
\land \ldots \land C_{\addr{n}}$. The problem of estimating the total leaked information 
can be reduced to the problem of counting the number of different solutions 
that satisfies the constraint $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$. 

A native method for approximating 
the result is to pick $k$ elements from $K$ and check how many of them are also
contained in $K^o$. If $q$ elements are also in $K^o$. In expectation, we can
use $\frac{k}{q}$ to approximate the value of $\frac{|K|}{|K^o|}$.

However, as the discussion in ~\ref{MCreasons},
the above sampling method will typically fail in practice due to the following two problems:

\begin{enumerate}
      \item The curse of dimensionality. $F(c_1,\ldots,c_n)$ is the conjunction of many
      constraints. Therefore, the input variables of each constraints will also be 
      the input variables of the  $F(c_1,\ldots,c_n)$. The sampling method will fail as 
      $n$ increases. For example, when $n$ equals to 2, the whole serach space is 
      a $256^2$ cube. If we want the sampling distance between each point still to 1,
      we need $256^2$ points. When $n$ equals to 10, we need $256^{10}$ points if we 
      still we want the distance between each points equals to 1. 

      \item The number of satisfying assignments could be exponentially small.
      According to Chernoff bound, we need exponentially many samples to get 
      a tight bound. On an extreme situation, if the constraint only has one unique
      satisfying solution, the simple Monte Carlo method can't find the satisfying
      assignment after sampling many points.
\end{enumerate}

However, despite the two problems. We also observe the constrain $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$
has the following characteristics:
\begin{enumerate}
      \item $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$ is the conjunction of several
      short constraints $F(c_{{addr}_i})$. The set contaning the input variables of 
      $F(c_{{addr}_i})$ is the subset of the input variables of $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$.
      Some constraints have completely different input variables from other constraints.
      \item For each constraint $F(c_{{addr}_i})$, the satisfying assignments
      are close to each other, which means if we find one satisfying assignment, we 
      are more likely to find other satisfying assignments nearby than randomly
      pick one point in the whole searching space.
      
\end{enumerate}

In regard to the above problems, we present our methods. First, we split 
$F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$ into several independent constraint groups. After
that, we run random-walk based sampling method on each constraint.

\subsubsection{Maximum Independent Partition}

For a constraint $C_i$, we define the function $S$, which maps
the constraint into the set consisting of input symbols. For example, 
$S(k1 + k2 > 128) = \{k1, k2\}$.

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{definition}[]
      \label{independentC}
      Given two constraints $C_m$ and $C_n$, we call them independent iff 
      $$S(C_m) \cap S(C_n) = \emptyset$$
\end{definition}

Based on the definition~\label{independentC}, we can split
the constraint $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$ into several 
independent constraints. There are many partitions. For our project, 
we are interested in the following one.

\begin{definition}
      \label{Goodpartition}
      For the constraint $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$ and 
      the constraint group
      $C_{g1}, C_{g2}, \ldots, C_{gm}$, we call  $C_{g1}, C_{g2}, \ldots, C_{gm}$
      is the maximum independent partition of $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$ iff
      \begin{enumerate}
            \item $C_{g1} \land C_{g2} \land \ldots \land C_{gm} = F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$
            \item $\forall \quad i, j \in \{1, 2, 3, \ldots, m\} \quad \textrm{and} \quad 
                  i \neq j, \quad S(C_{gi}) \cap S(C_{gj}) = \emptyset $
            \item For any other partitions  $C_{g1}, C_{g2}, \ldots, C_{gn}$ satisfy 1) and
                  2), $m \geq n$    
      \end{enumerate}
      
\end{definition}

The reason we want a good partition of the constraints is that we want to 
reduce the dimensions. Consider the example in the previous section,
$$F: {k_1} = 1\land{k_2} = 2\land{k_3} = 3\land{k_4} = 4$$
The good partition of $F$ would be
$$C_{g1}: {k_1} = 1\quad C_{g2}: {k_2} = 2\quad C_{g3}: {k_3} = 3\quad C_{g4}: {k_4} = 4$$     
So instead of sampling in the four dimension space, we can
sample each constraint in the one dimension space and combine them
together with~\ref{mip-theorem} .
\newtheorem{theorem}{Theorem}[section]
\label{mip-theorem}
\begin{theorem}
      \label{IndependentConstraint}
$C_{g1}, C_{g2}, \ldots, C_{gm}$ is a maximum independent partition of $F(c_1,c_2,\ldots,c_n)$.
$K_c$ is the input set that satisfies constrain $c$. We can have the following
equation in regard to the size of $K_c$
$$|K_{F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})}| = |K_{C_{g1}}|*|K_{C_{g2}}|*\ldots*|K_{C_{gn}}|$$
\end{theorem}

With the~\ref{IndependentConstraint}, we can transfer the problem of counting the number of 
solutions in a large constraint with high
dimensions into counting solutions of 
several small constraints. We apply the following algorithm to get the Maximum Independent Partition
of the $F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}})$.

\IncMargin{1em}
\begin{algorithm}[h]
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{$F(c_{\addr{1}},c_{\addr{2}},\ldots,c_{\addr{n}}) = C_{\addr{1}} \land C_{\addr{2}} \land \ldots \land C_{\addr{m}}$}
\Output{The Maximum Independent Partition of $G = \{C_{g1}, C_{g2}  , \ldots,  C_{gm} \}$ }
\For{$i\leftarrow 1$ \KwTo $n$}
{
   $S_i$ $\leftarrow$ $S(C_{\addr{i}})$ \;
   \For{$C_{gi} \in G$} 
   {
   $S_{gi}$ $\leftarrow$ $S(C_{gi})$ \;
   $s$ $\leftarrow$ $S_i \cap S_{gi}$  \;
   \If{$s \neq \emptyset$}
   {
      $C_{gi} = C_{gi} \land C_{\addr{i}}$ \;
      $break$ \;
   }
   Insert $C_{\addr{i}}$ to $G$
   }
}
\caption{The Maximum Independent Partition}
\end{algorithm}
\DecMargin{1em}

\subsubsection{Multiple Step Monte Carlo Sampling}

After we split those constraints into several small constraints, we count
the number of solutions for each constraint. Even though the dimension
has been reduced greatly after the previous step, this is still a
\#P problem. For our project, we apply the approximate counting instead of
exact counting for two reasons. First, we don't need to have a very precise
result of the exact number of total solutions. The information is defined with
a logarithmic function. We don't need to distinguish between a constraint having
$10^{10}$ and $10^{10} + 10$ solutions.
Second, as the constraint could be very complicated. The exact model counting
approaches, like DPLL search, have difficulty scaling up to large problem sizes.

We apply the ``counting by sampling" method. The basic idea is as follows.
For the constraint $C_{gi}= C_{i_1} \land C_{i_2} \land ,\ldots, \land C_{i_j} \land ,\ldots, 
\land C_{i_m}$, 
if the solution satisfies $C_{gi}$, it also
satisfies any constraint from $C_{i_1}$ to $C_{i_m}$. In other words,
$K_{C_gi}$ should be the subset of $K_{C_1}$, $K_{C_2}$, \ldots , $K_{C_m}$.
We notice that $C_i$ usually has less numbers of input compared to $C_{gi}$.
For example, if $C_{i_j}$ has only one input variable, we can find the exact
solution set $K_{C_{i_j}}$ of $C_{i_j}$ by trying every possible 256 solutions. After that,
we can only generate random input numbers for the rest input variables in 
constraint $C_{gi}$. With the simple trick, we can reduce the number of input while
still ensure the accuracy.

%% the algorithm
\IncMargin{1em}
\begin{algorithm}
\SetAlgoLined
\DontPrintSemicolon

\KwIn{{The constraint $C_{gi}= C_{i_1} \land C_{i_2}
\land \ldots \land C_{i_m}$}}    
\KwOut{{The number of assignments that satisfy $C_{gi}$ $|K_{C_{gi}}|$}}

\SetKwProg{RW}{RandomWalk}{}{}
\SetKwProg{MM}{MetropolisMove}{}{} 
$n$: the number of sampling times \;
$P$: a probability generator \;
$k$: the input assignment \;
$n_{s}$: the number of satisfying assignments \;
$SN_k$: the satisfying number of k \; 
Initialization: \;
$SN_{k_0}$ $\leftarrow$ $\sum_{j=1}^{m}C_{i_j}(k_0)$ \;
\For{$t\leftarrow 1$ \KwTo $n$} {
      $p$ $\leftarrow$ $P$ \;
      \If{$p \geq 0.5$}
      {
        $v$ $\leftarrow$ \RW{$v$} {}
      }
      \Else{
            $v$ $\leftarrow$ \MM{$v$} {}
      }
      \If{$v$ satisfies $C_{gi}$}
      {$n_{s}$ $\leftarrow$ $n_{s} + 1$}
}
$|K_{C_{gi}}|$ $\leftarrow$ $n_s|K| / n$
\caption{Metropolis Sampling}
\end{algorithm}
\DecMargin{1em}

\subsubsection{Error Estimation}
In this part, we analyze the accuracy of Monte Carlo approximate
result. We use central limit theorem (CLT) and propagation of uncertainty 
to estimate errors of the number of leaked bits for each site.

Let $n$ be the number of samples and $n_s$ be the number of samples
that satisfy the constraint $C$. Then we can get $\hat{p} = \frac{n_s}{n}$.
If we run the same experiment multiple times, each time we can
get a $p$. 
As each experiment is independent from others, according to 
central limit theorem, the sample mean $\bar{p}$ should 
satisfy normal distribution.
$$ \frac{\bar{p}-E(p)}{\sigma\sqrt{n}} \rightarrow N(0,1) $$
Here $E(p)$ is the mean value of $p$ and $\sigma$ is the standard 
variance of $p$. If we use the observed value $\hat{p}$ 
to the represent standard deviation.
We can claim that we have 95\% confidence that the error 
$\Delta p= \bar{p} - E(p)$ falls in the interval:
$$ |\Delta p| \leq 1.96\sqrt{\frac{ \hat{p} (1- \hat{p} )}{n}}$$

Since we use $L = \log_{2}p$ to estimate the amount of leaked information,
we can have the following error propagation formula $\Delta L = \frac{\Delta p}{pln2}$
by differentiation. For \tool, we want the error of estimated leaked information ($\Delta L$) is 
less than 1 bit. So we can get $\frac{\Delta p}{pln2} \leq 1$. As long as
$ n \geq \frac{1.96^2(1-p)}{p(\ln2)^2}$, we have 95\% confidence the error of estimated 
leaked information is less than 1 bit. During the simulation, if $n$ and $p$ satisfy
the above equation, the Monte Carlo Simulation will terminate.



