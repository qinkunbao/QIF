\section{Motivation}

\subsection{Techinical Chanllenges}
In this section, we articulate several chanllenges and existing problems
in quantifying the side-channel vulnerability leakages. We briefly describe the
chanllenges and then present the corresponding solutions.

\subsubsection{Information Leakage Definition}
Existing static-based side-channel quantification works defined information leakage
as the mutual information or the max leakage. Those definitions provide strong security guarantee
when trying to show a program is secure if the their methods say the program leaks zero bits of
information.

However, the above definition is less useful when if the program has some leakages. 
Considering the example in section ~\ref{code::entropy}, if an attacker observes the
code runs branch A, the attacker can know the key actually equals to 128. Suppose it is 
a dummy password checker, in which case the attacker can fully retrieve the password.
Therefore, the total information leakage should be 8 bits, which equals to the size
of unsigned char. 
According to the mutual definition, however, the leakage will be 1.7 bits. The maximal information
leakage is 2 bits. Both approaches fail to precisely tell how much information is actually leaked
during the exectution.

The problem with the existing method is that they are static-based and the sensitive
input values are neglected. They assume the attacker runs the program multiple times 
with many different sensitive information as the input. Both the mutual information 
and the max-leakage give an ``average" estimate of the information leakage. 
But it isn't the typical scenario for an adversary that launches the side-channel attack.
When a side-channel attack happens, the adversary wants to retrieve the sensitive
information. So it is very likely that the sensitive input is fixed (e.g. AES keys). 
The adversary will run the attack over and over again and guess bit by bit. Like the 
previous example, the existing static method doesn’t work well in the situations.

In the project, we hope to give a very precise definition of information leakages. 
Suppose an attacker run the target program mutiple times with one fixed input, we
want to how much information he can infer by oberserving the memory access patterns.
We come to the simple slogan ~\cite{10.1007/978-3-642-00596-1_21} where the information
leakage equals,

\textit{initial uncertainty - remaining uncertainty}

If an adversary has zero knowledge about the input before the attack. The initial uncertainty
equals to the size the input. As for the remaining uncertainty, we come to the original definition
of the information content.

\newtheorem{mydef}{Definition}

\begin{mydef}
Given a program $P$ with the input set $K$, 
an adversary has the observation $o$ when the input $k{\in}K$. 
We denote it as
    $$P(k) = o$$
The leakage $L_{Pko}$ based on the oberservation is
    $$L_{Pko} = log_2{|K|} - log_2{|K^o|}$$
    where
    $$K^o = \{k^{'} | k^{'}{\in}K \ and \ P(k^{'}) = o \}$$
\end{mydef}

With the new definition, if the attacker observes the branch 1 was executed, then the
$K^{o^{1}} = \{128\}$. Therefore, the information leakages $L_{Pko^{1}} = log_2{256} - log_2{1} = 8$
bits, which means the key is totally leaked. If the attacker observes the code runs other
branches, the leaked information is shown in the following table.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Branch & 1 & 2  & 3  & 4   \\ \hline
    $K^o$   & 1 & 64 & 64 & 127 \\ \hline
    $L_{Pko}$(bits)   & 8 & 2  & 2  & 1   \\ \hline
    \end{tabular}
    \caption{The leaked information by the definition}
\end{table}

\subsection{Multiple Leakage Sites}
Real-world software can have various side-channel vulnerabilities. Those vulnerabilities 
may spread in the whole program. An adversary may exploit more than one side-channel vulnerabilities 
to gain more information. For example, the controlled-side channel attack~\cite{7163052, 191010}, the author 
demonstrates an attack against a popular spell checking tool, Hunspell. By observing four sets 
of secret-dependent memory accesses sites in two functions $HashMgr::addword$ and $HashMgr::lookup$, 
the author can recover the word that Hunspell checks.

For the Hunspell, the attacker manually study the source code of Hunspell, figure out
the relation of those vulnerabilities and launch the attack. In order to precisely quantify the
total information leakage, we need to know the relation of those leakage sites. 

Suppose one program has two side-channel vulnerabilities A and B, which leaks $L_A$ and $L_B$ bits
during the execution. The total information leakage is noted as $L_{Total}$. The relation between
A and B has the following three cases.

\subsubsection{Independent Leakages}
If A and B are independent leakages, the total information leakage will be:
\begin{equation}
\label{independent leakage}
    L_{total} = L_A + L_B \nonumber
\end{equation} 

\subsubsection{Dependent Leakages}
If A and B are dependent leakages, the total information leakage will be:
\begin{equation}
\label{dependent leakage}
    \max{\{L_A, L_B\}}  <= L_{total} < L_A + L_B \nonumber
\end{equation}

\subsubsection{Mutual Exclusive Leakages}
If A and B are mutual exclusive leakges, then only A or B can be observed for one fixed input.
The total information leakage will be $L_A$ or $L_B$.

\lstinputlisting[language=c, 
                 numbers=left,
                 numbersep=5pt,                   % how far the line-numbers are from the code
                 caption={Multiple leakages},
                 %frame = simple,
                 captionpos=b,
                 label={code::entropy},
                 basicstyle=\fontsize{7}{9}\selectfont\ttfamily]
                 {sample_code/dependent.c}
For a real program, it is very hard to estimate the total information leakage for the following reasons.
First, the real-world applications have more than thousands of lines of code. Side-channel vulnerabilities
could exist in many different functions of the source code. 

We run the symbolic execution on the top of the execution traces. We use symbols
to represent each byte in the input buffer. 
\subsection{Scalability and Performance}

We use the mutual information (MI) to quantify the information leakages. 
For a program P with sensitive information K as the input, the attacker may have some observations O during the execution. 
The information leakage is defined as the mutual information I(O; K) between O and K.
\begin{equation}
I(O; K) = H(O) - H(O|K)
\end{equation}

I(O; K) represent how much uncertainty about K can be reduced if the attacker has the observation O.
For a deterministic program, the program will have the same memory access behavior as long as the input is fixed. 
As the observation of the attacker correlate to the memory access behavior, 
we can have the following formula.
\begin{equation}
O = f(K)
\end{equation}

The function f is determined by the program P. For our project, we can calculate the function f via symbolic execution.
\begin{equation}
I(O; K) = H(O) - H(f(K)|K) = H(O) = H(f(K))
\end{equation}

So the mutual information between O and S equals to the self information of O. 
\begin{equation}
H(O) = Σp(Oi)log(p(Oi))
\end{equation}

For a determinist program, we can calculate the distribution of O as long as we know the distribution of input K. 
So we can calculate how much information is leaked.

For examples, given a program P, we have the sensitive input K. The K should be a value in a memory cell or a sequential buffer (e.g., an array). We use ki to denote the sensitive information, where i is the index of the byte in the original buffer.  We can have the following equations. The t1, t2, t3, is the temporary values during the execution.

$$t_1 = f1(k1, k2, ... kn)$$
$$t_2 = f2(k1, k2, ... kn)$$
$$t_3 = f3(k1, k2, ... kn)$$
$$tm = fm(k1, k2, ... kn)$$


The attacker can retrieve the sensitive information by observing the different patterns in control-flows and data access when the program process different sensitive information. We refer them as the secret-dependent control flow and secret-dependent data access accordingly.

\subsection{Secret-dependent Control Flow}
Here is an example of the secret-dependent control-flows. Consider the code snippet in List 1. Here the key is the confidential data. The code will have different behaviours (time, cache access) dependenting on which branch is actually executing. By observing the behaviour, the attacker can infer which branch actually executed and know some of the sensitive information. One of the famous leakage example is the square and multiply in many RSA implementations. 

For example, the attacker knows the key equals to zero if he observes the code run the branch1. Because key has 256 different possibilities. The original key has lg256 = 8 bits information. If the attacker can observe the code run branch 1. Then he will knows the key equals to zero. If the code run branch 2, the attacker can infer the key doesn’t equal to zero. 

Branch 1
temp = 0xb;
0 =< key <= 256;
temp = key/2;

Information Leakage = -log(1/p) = -log(1/256) = 8 bits

Branch 2
temp != 0; 
0 =< key <= 256;
temp = key/2;

Information Leakage = -log(255/256) bits

\subsection{Seret-dependent Memory Access}

\begin{lstlisting}

T[64]; // Lookup tables with 64 entries
index = key % 63;
temp = T[index]; 
// Secret-dependent memory access       

\end{lstlisting}

The simple program above is an example of secret dependent memory access. Here T is a precomputed tables with sixty-four entries. Depending on the values of key, the program may access any values in the array. Those kind of code patterns may wildly exist in many crypto and media libraries. 

Suppose the attackers observe the code accesses the first entry of the lookup tables. We can have the following formulas.

key mod 63 ≡ 1
0 =< key <= 256

So the key can be one of the following values:
1 64 127 190 253

Information leakages = -log(5/256) =  5.6 bits

