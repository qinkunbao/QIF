
\section{Information Leakage Quantification}
 


Given an event $e$ which occurs with the probability $p(e)$, if the event $e$ happens, 
then we receive
\begin{displaymath}
    I = - \log_2p(e)
\end{displaymath}
bits of information by knowing the event $e$.
Considering a char variable $a$ with one byte storage size in a C program, its value 
ranges from 0 to 255.  Assume
 \textit{a} has the uniform distribution. If at one time we observe that $a$
equals $1$, the probability of this observation is $\frac{1}{256}$. So the information we get is 
$-\log(\frac{1}{256}) = 8$ bits, which is exactly the size of the char variable in the C program.

Existing works on information leakage quantification uses Shannon entropy.
min-entropy \cite{10.1007/978-3-642-00596-1_21} and max-entropy.
In these frameworks, the input sensitive
information $K$ is viewed as a random variable. 

Let $k_i$ be one of the possible
value of $K$. The Shannon entropy $H(K)$ is defined as
\begin{displaymath}
    H(K) = - \sum_{k_i {\in} K}p(k_i)\log_2(k_i)
\end{displaymath}

Shannon entropy can be used to quantify the initial uncertainty about the sensitive
information. It measures how much information is in a system.

Min-entropy describes in the information leaks from the most likely outcome.
Min-entropy can be used to describe the best chance of success in guessing 
one's password in one chance, which is defined as 
\begin{displaymath}
    min-entropy = - log_2(p_{max})
\end{displaymath}

Max-entropy is defined solely on the number of possible observations. It is 
equal to $-log_2{n}$. As it is easy to compute, most recent works use 
max-entropy as the definition of the amount of leaked information.

To illustrate, how the above definitions work, we consider the following
code. 

\begin{lstlisting}[xleftmargin=.03\textwidth,xrightmargin=.01\textwidth]
uint_8 key[2]; 
get_key(key);               // key[0], key[1] = [0 ... 255]
if(key[0] < 63){
    A();                    // branch 1
}
if(key[0] > key[1]){        // branch 2
    B();
}
\end{lstlisting}

Suppose an attacker can observe if branch 1 and branch 2 are executed or not, 
an attacker can have four different observations: neither branch 1 or 
branch 2 is executed $\emptyset$, branch 1 is executed ${1}$, branch 2
is executed ${2}$, and both branch 1 and branch 2 are executed ${1, 2}$.

%% put this example in next section
%%\section{Running Example}

%Now we provide a concrete example to show how the two types of quantification definition works and show that
%how our method is different.

%\vspace{3pt}
%\textbf{Maximal Leakage.} 
%Depending on the value of key, the code can run four different branches which corrosponding to 
%four different observations. Therefore, by the maximal leakage definition, the leakage equals to 
%$\log4 = 2$ bits.

%\vspace{3pt}
%\textbf{Mutual Information.}
%If the key satisfies the uniform distribution, the probability of the code runs each branch
%can be computed with the following result.  

%\begin{table}[h]
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{|c|c|c|c|c|}
%\hline
%Branch & A     & B      & C      & D       \\ \hline
%Possibility      & 1/256 & 64/256 & 64/256 & 127/256 \\ \hline
%\end{tabular}
%}
%\caption{The distribution of observations}
%\end{table}

%Therefore, the leakage equals to 
%$\frac{1}{256}\log\frac{1}{256} + \frac{1}{4}\log\frac{1}{4}*2 + \frac{127}{256}\log\frac{127}{256} = 1.7$ bits.

%\vspace{3pt}
%\textbf{\tool.}
%\fixme{What is out result here? Need explain}
